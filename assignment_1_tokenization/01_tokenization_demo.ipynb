{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ๐ ุงูุชูููู ุงูุฃูู: ุฅุนุฏุงุฏ ุงูุจูุฆุฉ ูุชูุณูู ุงููููุงุช (Tokenization)\n",
    "\n",
    "## ๐ฏ ุงูุฃูุฏุงู\n",
    "1. ุชุฌููุฒ ุงูุจูุฆุฉ ูุชุซุจูุช ุงูููุชุจุงุช ุงููุงุฒูุฉ\n",
    "2. ุดุฑุญ Tokenization ุจุงุณุชุฎุฏุงู ููุชุจุฉ Spacy\n",
    "3. ุชูุถูุญ ุงูููุฑุฉ ูุงูุฎุทูุงุช ูุงูุฃุฏุงุฉ ุงููุณุชุฎุฏูุฉ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1๏ธโฃ ุชุฌููุฒ ุงูุจูุฆุฉ\n",
    "\n",
    "### ุชุซุจูุช ุงูููุชุจุงุช ุงููุทููุจุฉ\n",
    "ูู ุจุชุดุบูู ุงูุฃูุงูุฑ ุงูุชุงููุฉ ูู Terminal ูุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงููุงุฒูุฉ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ุชุซุจูุช ุงูููุชุจุงุช ุงูุฃุณุงุณูุฉ (ูู ุจุชุดุบูู ูุฐุง ูู Terminal ุฅุฐุง ูู ุชูู ูุซุจุชุฉ)\n",
    "# !pip install spacy nltk camel-tools pyarabic qalsadi transformers torch pandas scikit-learn\n",
    "\n",
    "# ุชุญููู ููุงุฐุฌ Spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download xx_ent_wiki_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ Spacy version: 3.8.11\n",
      "โ NLTK version: 3.9.2\n",
      "โ Pandas version: 2.3.3\n",
      "โ Scikit-learn version: 1.7.2\n",
      "\n",
      "๐ ุฌููุน ุงูููุชุจุงุช ุงูุฃุณุงุณูุฉ ูุซุจุชุฉ ุจูุฌุงุญ!\n"
     ]
    }
   ],
   "source": [
    "# ุงูุชุญูู ูู ุชุซุจูุช ุงูููุชุจุงุช\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "print(\"โ Spacy version:\", spacy.__version__)\n",
    "print(\"โ NLTK version:\", nltk.__version__)\n",
    "print(\"โ Pandas version:\", pd.__version__)\n",
    "print(\"โ Scikit-learn version:\", sklearn.__version__)\n",
    "print(\"\\n๐ ุฌููุน ุงูููุชุจุงุช ุงูุฃุณุงุณูุฉ ูุซุจุชุฉ ุจูุฌุงุญ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2๏ธโฃ ุดุฑุญ Tokenization ุจุงุณุชุฎุฏุงู Spacy\n",
    "\n",
    "### ๐ ูุง ูู Tokenizationุ\n",
    "\n",
    "**Tokenization** (ุชูุณูู ุงููููุงุช) ูู ุนูููุฉ ุชุญููู ุงููุต ุฅูู ูุญุฏุงุช ุฃุตุบุฑ ุชุณูู **Tokens**. \n",
    "ูุฐู ุงูุนูููุฉ ูู ุงูุฎุทูุฉ ุงูุฃููู ูุงูุฃุณุงุณูุฉ ูู ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ.\n",
    "\n",
    "### ๐ก ุงูููุฑุฉ\n",
    "\n",
    "- ุชูุณูู ุงููุต ุงููุงูู ุฅูู ูููุงุช ุฃู ุฑููุฒ ูููุตูุฉ\n",
    "- ูู Token ููุซู ูุญุฏุฉ ูุบููุฉ (ูููุฉุ ุนูุงูุฉ ุชุฑูููุ ุฑููุ ุฅูุฎ)\n",
    "- Spacy ูุง ูููู ููุท ุจุงูุชูุณููุ ุจู ูุถูู ูุนูููุงุช ูุบููุฉ ููู Token\n",
    "\n",
    "### ๐๏ธ ุงูุฃุฏุงุฉ ุงููุณุชุฎุฏูุฉ: Spacy\n",
    "\n",
    "Spacy ูู ููุชุจุฉ ูููุฉ ููุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉุ ุชุชููุฒ ุจู:\n",
    "- ุณุฑุนุฉ ุนุงููุฉ ูู ุงููุนุงูุฌุฉ\n",
    "- ุฏุนู ูุบุงุช ูุชุนุฏุฏุฉ\n",
    "- ุชูููุฑ ูุนูููุงุช ูุบููุฉ ุบููุฉ (POS, NER, Dependencies)\n",
    "- ุณูููุฉ ุงูุงุณุชุฎุฏุงู"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ๐ ุงูุฎุทูุงุช ุงูุชุทุจูููุฉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ ุชู ุงุณุชูุฑุงุฏ ููุชุจุฉ Spacy ุจูุฌุงุญ\n"
     ]
    }
   ],
   "source": [
    "# ุงูุฎุทูุฉ 1: ุงุณุชูุฑุงุฏ ููุชุจุฉ Spacy\n",
    "import spacy\n",
    "\n",
    "print(\"โ ุชู ุงุณุชูุฑุงุฏ ููุชุจุฉ Spacy ุจูุฌุงุญ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ ุชู ุชุญููู ุงููููุฐุฌ ูุชุนุฏุฏ ุงููุบุงุช ุจูุฌุงุญ\n"
     ]
    }
   ],
   "source": [
    "# ุงูุฎุทูุฉ 2: ุชุญููู ุงููููุฐุฌ ุงููุบูู\n",
    "# ูุณุชุฎุฏู ุงููููุฐุฌ xx_ent_wiki_sm ุงูุฐู ูุฏุนู ูุบุงุช ูุชุนุฏุฏุฉ ุจูุง ูููุง ุงูุนุฑุจูุฉ\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "    print(\"โ ุชู ุชุญููู ุงููููุฐุฌ ูุชุนุฏุฏ ุงููุบุงุช ุจูุฌุงุญ\")\n",
    "except:\n",
    "    # ุฅุฐุง ูู ููู ุงููููุฐุฌ ููุฌูุฏุงูุ ุงุณุชุฎุฏู ุงููููุฐุฌ ุงูุฅูุฌููุฒู\n",
    "    print(\"โ๏ธ ุฌุงุฑู ุชุญููู ุงููููุฐุฌ...\")\n",
    "    !python -m spacy download xx_ent_wiki_sm\n",
    "    nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "    print(\"โ ุชู ุชุญููู ุงููููุฐุฌ ุจูุฌุงุญ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ ุงููุต ุงูุนุฑุจู:\n",
      "ูุฑุญุจุงู ุจูู ูู ููุฑุฑ ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ! ูุฐุง ุงูููุฑุฑ ูุนูููุง ููููุฉ ุงูุชุนุงูู ูุน ุงููุตูุต.\n",
      "\n",
      "๐ ุงููุต ุงูุฅูุฌููุฒู:\n",
      "Welcome to Natural Language Processing course! This course teaches us how to handle texts.\n"
     ]
    }
   ],
   "source": [
    "# ุงูุฎุทูุฉ 3: ุชุฌููุฒ ุงููุต ุงููุฑุงุฏ ุชูุณููู\n",
    "\n",
    "# ูุต ุนุฑุจู ููุชุฌุฑุจุฉ\n",
    "arabic_text = \"ูุฑุญุจุงู ุจูู ูู ููุฑุฑ ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ! ูุฐุง ุงูููุฑุฑ ูุนูููุง ููููุฉ ุงูุชุนุงูู ูุน ุงููุตูุต.\"\n",
    "\n",
    "# ูุต ุฅูุฌููุฒู ููููุงุฑูุฉ\n",
    "english_text = \"Welcome to Natural Language Processing course! This course teaches us how to handle texts.\"\n",
    "\n",
    "print(\"๐ ุงููุต ุงูุนุฑุจู:\")\n",
    "print(arabic_text)\n",
    "print(\"\\n๐ ุงููุต ุงูุฅูุฌููุฒู:\")\n",
    "print(english_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ค ุงูู Tokens ุงููุงุชุฌุฉ ูู ุงููุต ุงูุนุฑุจู:\n",
      "['ูุฑุญุจุงู', 'ุจูู', 'ูู', 'ููุฑุฑ', 'ูุนุงูุฌุฉ', 'ุงููุบุงุช', 'ุงูุทุจูุนูุฉ', '!', 'ูุฐุง', 'ุงูููุฑุฑ', 'ูุนูููุง', 'ููููุฉ', 'ุงูุชุนุงูู', 'ูุน', 'ุงููุตูุต', '.']\n",
      "\n",
      "๐ ุนุฏุฏ ุงูู Tokens: 16\n"
     ]
    }
   ],
   "source": [
    "# ุงูุฎุทูุฉ 4: ุชุทุจูู Tokenization ุนูู ุงููุต\n",
    "\n",
    "# ูุนุงูุฌุฉ ุงููุต ุงูุนุฑุจู\n",
    "doc_ar = nlp(arabic_text)\n",
    "\n",
    "# ุงุณุชุฎุฑุงุฌ ุงูู Tokens\n",
    "tokens_ar = [token.text for token in doc_ar]\n",
    "\n",
    "print(\"๐ค ุงูู Tokens ุงููุงุชุฌุฉ ูู ุงููุต ุงูุนุฑุจู:\")\n",
    "print(tokens_ar)\n",
    "print(f\"\\n๐ ุนุฏุฏ ุงูู Tokens: {len(tokens_ar)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ ูุนูููุงุช ุชูุตูููุฉ ุนู ูู Token:\n",
      "------------------------------------------------------------\n",
      "Token           Index    Is Alpha   Is Punct   Is Space  \n",
      "------------------------------------------------------------\n",
      "ูุฑุญุจุงู          0        False      False      False     \n",
      "ุจูู             1        True       False      False     \n",
      "ูู              2        True       False      False     \n",
      "ููุฑุฑ            3        True       False      False     \n",
      "ูุนุงูุฌุฉ          4        True       False      False     \n",
      "ุงููุบุงุช          5        True       False      False     \n",
      "ุงูุทุจูุนูุฉ        6        True       False      False     \n",
      "!               7        False      True       False     \n",
      "ูุฐุง             8        True       False      False     \n",
      "ุงูููุฑุฑ          9        True       False      False     \n",
      "ูุนูููุง          10       True       False      False     \n",
      "ููููุฉ           11       True       False      False     \n",
      "ุงูุชุนุงูู         12       True       False      False     \n",
      "ูุน              13       True       False      False     \n",
      "ุงููุตูุต          14       True       False      False     \n",
      ".               15       False      True       False     \n"
     ]
    }
   ],
   "source": [
    "# ุงูุฎุทูุฉ 5: ุนุฑุถ ูุนูููุงุช ุชูุตูููุฉ ุนู ูู Token\n",
    "\n",
    "print(\"๐ ูุนูููุงุช ุชูุตูููุฉ ุนู ูู Token:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Token':<15} {'Index':<8} {'Is Alpha':<10} {'Is Punct':<10} {'Is Space':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for token in doc_ar:\n",
    "    print(f\"{token.text:<15} {token.i:<8} {str(token.is_alpha):<10} {str(token.is_punct):<10} {str(token.is_space):<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ค ุงูู Tokens ุงููุงุชุฌุฉ ูู ุงููุต ุงูุฅูุฌููุฒู:\n",
      "['Welcome', 'to', 'Natural', 'Language', 'Processing', 'course', '!', 'This', 'course', 'teaches', 'us', 'how', 'to', 'handle', 'texts', '.']\n",
      "\n",
      "๐ ุนุฏุฏ ุงูู Tokens: 16\n"
     ]
    }
   ],
   "source": [
    "# ุงูุฎุทูุฉ 6: ููุงุฑูุฉ ูุน ุงููุต ุงูุฅูุฌููุฒู\n",
    "\n",
    "doc_en = nlp(english_text)\n",
    "tokens_en = [token.text for token in doc_en]\n",
    "\n",
    "print(\"๐ค ุงูู Tokens ุงููุงุชุฌุฉ ูู ุงููุต ุงูุฅูุฌููุฒู:\")\n",
    "print(tokens_en)\n",
    "print(f\"\\n๐ ุนุฏุฏ ุงูู Tokens: {len(tokens_en)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3๏ธโฃ ุฃูุซูุฉ ุฅุถุงููุฉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ ุฃูุซูุฉ ุนูู Tokenization ููุตูุต ูุชููุนุฉ:\n",
      "============================================================\n",
      "\n",
      "๐ ุงููุซุงู 1: ุงูุฎุฏูุฉ ููุชุงุฒุฉ ุฌุฏุงู ๐\n",
      "๐ค Tokens: ['ุงูุฎุฏูุฉ', 'ููุชุงุฒุฉ', 'ุฌุฏุงู', '๐']\n",
      "----------------------------------------\n",
      "\n",
      "๐ ุงููุซุงู 2: ุงูุณุนุฑ: 100 ุฑูุงู ูููู\n",
      "๐ค Tokens: ['ุงูุณุนุฑ', ':', '100', 'ุฑูุงู', 'ูููู']\n",
      "----------------------------------------\n",
      "\n",
      "๐ ุงููุซุงู 3: ุฃุญูุฏ@email.com - ุฑูู ุงููุงุชู: 777123456\n",
      "๐ค Tokens: ['ุฃุญูุฏ@email.com', '-', 'ุฑูู', 'ุงููุงุชู', ':', '777123456']\n",
      "----------------------------------------\n",
      "\n",
      "๐ ุงููุซุงู 4: ูุง ุฃูุตุญ ุจูุฐุง ุงูููุชุฌ... ุณูุก ุฌุฏุงุงุงุง!!!\n",
      "๐ค Tokens: ['ูุง', 'ุฃูุตุญ', 'ุจูุฐุง', 'ุงูููุชุฌ', '...', 'ุณูุก', 'ุฌุฏุงุงุงุง', '!', '!', '!']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ูุซุงู ุนูู ูุตูุต ูุชููุนุฉ\n",
    "\n",
    "examples = [\n",
    "    \"ุงูุฎุฏูุฉ ููุชุงุฒุฉ ุฌุฏุงู ๐\",\n",
    "    \"ุงูุณุนุฑ: 100 ุฑูุงู ูููู\",\n",
    "    \"ุฃุญูุฏ@email.com - ุฑูู ุงููุงุชู: 777123456\",\n",
    "    \"ูุง ุฃูุตุญ ุจูุฐุง ุงูููุชุฌ... ุณูุก ุฌุฏุงุงุงุง!!!\"\n",
    "]\n",
    "\n",
    "print(\"๐ ุฃูุซูุฉ ุนูู Tokenization ููุตูุต ูุชููุนุฉ:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(examples, 1):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    print(f\"\\n๐ ุงููุซุงู {i}: {text}\")\n",
    "    print(f\"๐ค Tokens: {tokens}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4๏ธโฃ ุฏุงูุฉ Tokenization ุฌุงูุฒุฉ ููุงุณุชุฎุฏุงู"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ค Tokens ุจุณูุทุฉ:\n",
      "['ูุฑุญุจุง', 'ุจูู', 'ูู', 'ุงูููู', 'ุงูุณุนูุฏ', '!']\n",
      "\n",
      "๐ Tokens ูุน ูุนูููุงุช:\n",
      "{'text': 'ูุฑุญุจุง', 'index': 0, 'is_alpha': True, 'is_punct': False, 'is_space': False, 'is_digit': False}\n",
      "{'text': 'ุจูู', 'index': 1, 'is_alpha': True, 'is_punct': False, 'is_space': False, 'is_digit': False}\n",
      "{'text': 'ูู', 'index': 2, 'is_alpha': True, 'is_punct': False, 'is_space': False, 'is_digit': False}\n",
      "{'text': 'ุงูููู', 'index': 3, 'is_alpha': True, 'is_punct': False, 'is_space': False, 'is_digit': False}\n",
      "{'text': 'ุงูุณุนูุฏ', 'index': 4, 'is_alpha': True, 'is_punct': False, 'is_space': False, 'is_digit': False}\n",
      "{'text': '!', 'index': 5, 'is_alpha': False, 'is_punct': True, 'is_space': False, 'is_digit': False}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text, nlp_model=None, return_info=False):\n",
    "    \"\"\"\n",
    "    ุฏุงูุฉ ูุชูุณูู ุงููุต ุฅูู Tokens ุจุงุณุชุฎุฏุงู Spacy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        ุงููุต ุงููุฑุงุฏ ุชูุณููู\n",
    "    nlp_model : spacy.lang\n",
    "        ูููุฐุฌ Spacy (ุงุฎุชูุงุฑู)\n",
    "    return_info : bool\n",
    "        ุฅุฐุง Trueุ ูุฑุฌุน ูุนูููุงุช ุฅุถุงููุฉ ุนู ูู Token\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list : ูุงุฆูุฉ ุงูู Tokens ุฃู ูุงููุณ ุจูุนูููุงุช ุฅุถุงููุฉ\n",
    "    \"\"\"\n",
    "    if nlp_model is None:\n",
    "        nlp_model = spacy.load(\"xx_ent_wiki_sm\")\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    if return_info:\n",
    "        return [\n",
    "            {\n",
    "                'text': token.text,\n",
    "                'index': token.i,\n",
    "                'is_alpha': token.is_alpha,\n",
    "                'is_punct': token.is_punct,\n",
    "                'is_space': token.is_space,\n",
    "                'is_digit': token.is_digit\n",
    "            }\n",
    "            for token in doc\n",
    "        ]\n",
    "    else:\n",
    "        return [token.text for token in doc]\n",
    "\n",
    "\n",
    "# ุชุฌุฑุจุฉ ุงูุฏุงูุฉ\n",
    "test_text = \"ูุฑุญุจุง ุจูู ูู ุงูููู ุงูุณุนูุฏ!\"\n",
    "\n",
    "print(\"๐ค Tokens ุจุณูุทุฉ:\")\n",
    "print(tokenize_text(test_text, nlp))\n",
    "\n",
    "print(\"\\n๐ Tokens ูุน ูุนูููุงุช:\")\n",
    "for info in tokenize_text(test_text, nlp, return_info=True):\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ๐ ููุฎุต\n",
    "\n",
    "### ูุง ุชุนูููุงู:\n",
    "\n",
    "| ุงูููุทุฉ | ุงูุดุฑุญ |\n",
    "|--------|-------|\n",
    "| **ุงูููุฑุฉ** | Tokenization ูู ุชูุณูู ุงููุต ุฅูู ูุญุฏุงุช ูุบููุฉ ุตุบูุฑุฉ (Tokens) |\n",
    "| **ุงูุฃุฏุงุฉ** | Spacy - ููุชุจุฉ ูููุฉ ูุณุฑูุนุฉ ููุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ |\n",
    "| **ุงูุฎุทูุงุช** | 1. ุชุญููู ุงููููุฐุฌ 2. ูุนุงูุฌุฉ ุงููุต 3. ุงุณุชุฎุฑุงุฌ Tokens |\n",
    "| **ุงููุงุฆุฏุฉ** | ุชุฌููุฒ ุงููุต ูููุนุงูุฌุฉ ุงููุงุญูุฉ (Stemming, Classification, etc.) |\n",
    "\n",
    "### ุงูููุฏ ุงูุฃุณุงุณู:\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "doc = nlp(\"ุงููุต ููุง\")\n",
    "tokens = [token.text for token in doc]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**โ ุงูุชูู ุงูุชูููู ุงูุฃูู**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

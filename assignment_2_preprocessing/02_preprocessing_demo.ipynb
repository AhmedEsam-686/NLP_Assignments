{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# التكليف الثاني: تنظيف ومعالجة النصوص (Preprocessing)\n",
                "\n",
                "## الأهداف\n",
                "1. تنفيذ عمليات تنظيف النصوص العربية\n",
                "2. تطبيق Tokenization, Stemming, Lemmatization\n",
                "3. مقارنة بين المكتبات المختلفة وتحديد الأفضل"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## استيراد المكتبات"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "pyarabic installed\n",
                        "qalsadi installed\n",
                        "Libraries loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "import string\n",
                "import pandas as pd\n",
                "import json\n",
                "from time import time\n",
                "\n",
                "# NLP Libraries\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.stem.isri import ISRIStemmer\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "# Arabic Specific\n",
                "try:\n",
                "    import pyarabic.araby as araby\n",
                "    print(\"pyarabic installed\")\n",
                "except:\n",
                "    print(\"pyarabic not installed\")\n",
                "\n",
                "try:\n",
                "    from qalsadi.lemmatizer import Lemmatizer\n",
                "    print(\"qalsadi installed\")\n",
                "except:\n",
                "    print(\"qalsadi not installed\")\n",
                "\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "\n",
                "print(\"Libraries loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## مجموعة النصوص للتجربة"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "النصوص الاصلية:\n",
                        "======================================================================\n",
                        "1. مرحباااا بكم في اليمن السعيد الخدمة روووعة جدا\n",
                        "2. السعر 1500 ريال غالي جدا\n",
                        "3. الاكل لذيذ والجو رائع انصح الجميع بزيارة هذا المطعم\n",
                        "4. خدمةةةة سيئةةة جدااااا لن اعود مرة اخرى\n",
                        "5. تجربة ممتازة 100 شكرا للفريق\n",
                        "6. المنتج وصل متاخر ب 3 ايام والتغليف سيء\n",
                        "7. احببت الخدمه كثييييرا ساعود قريبا ان شاء الله\n",
                        "8. لا انصح ابدا التعامل سيء والاسعار مرتفعة جدا\n"
                    ]
                }
            ],
            "source": [
                "sample_texts = [\n",
                "    \"مرحباااا بكم في اليمن السعيد الخدمة روووعة جدا\",\n",
                "    \"السعر 1500 ريال غالي جدا\",\n",
                "    \"الاكل لذيذ والجو رائع انصح الجميع بزيارة هذا المطعم\",\n",
                "    \"خدمةةةة سيئةةة جدااااا لن اعود مرة اخرى\",\n",
                "    \"تجربة ممتازة 100 شكرا للفريق\",\n",
                "    \"المنتج وصل متاخر ب 3 ايام والتغليف سيء\",\n",
                "    \"احببت الخدمه كثييييرا ساعود قريبا ان شاء الله\",\n",
                "    \"لا انصح ابدا التعامل سيء والاسعار مرتفعة جدا\"\n",
                "]\n",
                "\n",
                "print(\"النصوص الاصلية:\")\n",
                "print(\"=\" * 70)\n",
                "for i, text in enumerate(sample_texts, 1):\n",
                "    print(f\"{i}. {text}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## دوال التنظيف (Cleaning Functions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "تم انشاء فئة التنظيف بنجاح!\n"
                    ]
                }
            ],
            "source": [
                "class ArabicTextCleaner:\n",
                "    \"\"\"\n",
                "    فئة متكاملة لتنظيف النصوص العربية\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.arabic_punctuations = ''.join(['`', '÷', '×', '؛', '<', '>', '(', ')', '*', '&', '^', '%', ']', '[', 'ـ', '،', '/', ':', '\"', '؟', '.', ',', \"'\", '{', '}', '~', '¦', '+', '|', '!', '…', '\"', '\"', '–'])\n",
                "        self.english_punctuations = string.punctuation\n",
                "        self.all_punctuations = self.arabic_punctuations + self.english_punctuations\n",
                "        \n",
                "        self.emoji_pattern = re.compile(\n",
                "            \"[\" \n",
                "            \"\\U0001F600-\\U0001F64F\"\n",
                "            \"\\U0001F300-\\U0001F5FF\"\n",
                "            \"\\U0001F680-\\U0001F6FF\"\n",
                "            \"\\U0001F1E0-\\U0001F1FF\"\n",
                "            \"\\U00002702-\\U000027B0\"\n",
                "            \"\\U000024C2-\\U0001F251\"\n",
                "            \"\\U0001F900-\\U0001F9FF\"\n",
                "            \"\\U00002600-\\U000026FF\"\n",
                "            \"\\U00002700-\\U000027BF\"\n",
                "            \"]+\", flags=re.UNICODE)\n",
                "    \n",
                "    def remove_emojis(self, text):\n",
                "        \"\"\"ازالة الايموجي\"\"\"\n",
                "        return self.emoji_pattern.sub('', text)\n",
                "    \n",
                "    def remove_punctuation(self, text):\n",
                "        \"\"\"ازالة علامات الترقيم\"\"\"\n",
                "        translator = str.maketrans('', '', self.all_punctuations)\n",
                "        return text.translate(translator)\n",
                "    \n",
                "    def remove_elongation(self, text):\n",
                "        \"\"\"ازالة المد والتكرار\"\"\"\n",
                "        return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
                "    \n",
                "    def remove_numbers(self, text):\n",
                "        \"\"\"ازالة الارقام\"\"\"\n",
                "        return re.sub(r'[0-9٠-٩]+', '', text)\n",
                "    \n",
                "    def remove_diacritics(self, text):\n",
                "        \"\"\"ازالة التشكيل\"\"\"\n",
                "        try:\n",
                "            return araby.strip_tashkeel(text)\n",
                "        except:\n",
                "            arabic_diacritics = re.compile(r'[\\u064B-\\u065F\\u0670]')\n",
                "            return arabic_diacritics.sub('', text)\n",
                "    \n",
                "    def normalize_arabic(self, text):\n",
                "        \"\"\"تطبيع الحروف العربية\"\"\"\n",
                "        text = re.sub(r'[إأآا]', 'ا', text)\n",
                "        text = re.sub(r'ة', 'ه', text)\n",
                "        text = re.sub(r'ى', 'ي', text)\n",
                "        return text\n",
                "    \n",
                "    def remove_extra_spaces(self, text):\n",
                "        \"\"\"ازالة المسافات الزائدة\"\"\"\n",
                "        return ' '.join(text.split())\n",
                "    \n",
                "    def clean_text(self, text, normalize=True):\n",
                "        \"\"\"تطبيق جميع عمليات التنظيف\"\"\"\n",
                "        text = self.remove_emojis(text)\n",
                "        text = self.remove_punctuation(text)\n",
                "        text = self.remove_elongation(text)\n",
                "        text = self.remove_numbers(text)\n",
                "        text = self.remove_diacritics(text)\n",
                "        if normalize:\n",
                "            text = self.normalize_arabic(text)\n",
                "        text = self.remove_extra_spaces(text)\n",
                "        return text\n",
                "\n",
                "\n",
                "cleaner = ArabicTextCleaner()\n",
                "print(\"تم انشاء فئة التنظيف بنجاح!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "تطبيق عمليات التنظيف:\n",
                        "======================================================================\n",
                        "\n",
                        "النص 1:\n",
                        "   قبل: مرحباااا بكم في اليمن السعيد الخدمة روووعة جدا\n",
                        "   بعد: مرحبا بكم في اليمن السعيد الخدمه روعه جدا\n",
                        "\n",
                        "النص 2:\n",
                        "   قبل: السعر 1500 ريال غالي جدا\n",
                        "   بعد: السعر ريال غالي جدا\n",
                        "\n",
                        "النص 3:\n",
                        "   قبل: الاكل لذيذ والجو رائع انصح الجميع بزيارة هذا المطعم\n",
                        "   بعد: الاكل لذيذ والجو رائع انصح الجميع بزياره هذا المطعم\n",
                        "\n",
                        "النص 4:\n",
                        "   قبل: خدمةةةة سيئةةة جدااااا لن اعود مرة اخرى\n",
                        "   بعد: خدمه سيئه جدا لن اعود مره اخري\n",
                        "\n",
                        "النص 5:\n",
                        "   قبل: تجربة ممتازة 100 شكرا للفريق\n",
                        "   بعد: تجربه ممتازه شكرا للفريق\n",
                        "\n",
                        "النص 6:\n",
                        "   قبل: المنتج وصل متاخر ب 3 ايام والتغليف سيء\n",
                        "   بعد: المنتج وصل متاخر ب ايام والتغليف سيء\n",
                        "\n",
                        "النص 7:\n",
                        "   قبل: احببت الخدمه كثييييرا ساعود قريبا ان شاء الله\n",
                        "   بعد: احببت الخدمه كثيرا ساعود قريبا ان شاء الله\n",
                        "\n",
                        "النص 8:\n",
                        "   قبل: لا انصح ابدا التعامل سيء والاسعار مرتفعة جدا\n",
                        "   بعد: لا انصح ابدا التعامل سيء والاسعار مرتفعه جدا\n"
                    ]
                }
            ],
            "source": [
                "# تطبيق التنظيف على النصوص\n",
                "print(\"تطبيق عمليات التنظيف:\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "cleaned_texts = []\n",
                "for i, text in enumerate(sample_texts, 1):\n",
                "    cleaned = cleaner.clean_text(text)\n",
                "    cleaned_texts.append(cleaned)\n",
                "    print(f\"\\nالنص {i}:\")\n",
                "    print(f\"   قبل: {text}\")\n",
                "    print(f\"   بعد: {cleaned}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization (تقسيم الكلمات)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "مقارنة طرق Tokenization:\n",
                        "\n",
                        "النص: مرحبا بكم في اليمن السعيد الخدمه روعه جدا\n",
                        "\n",
                        "1. Spacy: ['مرحبا', 'بكم', 'في', 'اليمن', 'السعيد', 'الخدمه', 'روعه', 'جدا']\n",
                        "2. NLTK: ['مرحبا', 'بكم', 'في', 'اليمن', 'السعيد', 'الخدمه', 'روعه', 'جدا']\n",
                        "3. Simple: ['مرحبا', 'بكم', 'في', 'اليمن', 'السعيد', 'الخدمه', 'روعه', 'جدا']\n"
                    ]
                }
            ],
            "source": [
                "# تحميل نموذج Spacy\n",
                "try:\n",
                "    nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
                "except:\n",
                "    nlp = spacy.blank(\"ar\")\n",
                "\n",
                "def tokenize_with_spacy(text):\n",
                "    \"\"\"Tokenization باستخدام Spacy\"\"\"\n",
                "    doc = nlp(text)\n",
                "    return [token.text for token in doc if not token.is_space]\n",
                "\n",
                "def tokenize_with_nltk(text):\n",
                "    \"\"\"Tokenization باستخدام NLTK\"\"\"\n",
                "    return word_tokenize(text)\n",
                "\n",
                "def tokenize_simple(text):\n",
                "    \"\"\"Tokenization بسيط\"\"\"\n",
                "    return text.split()\n",
                "\n",
                "# تطبيق على نص مثال\n",
                "test_text = cleaned_texts[0]\n",
                "\n",
                "print(\"مقارنة طرق Tokenization:\")\n",
                "print(f\"\\nالنص: {test_text}\")\n",
                "print(f\"\\n1. Spacy: {tokenize_with_spacy(test_text)}\")\n",
                "print(f\"2. NLTK: {tokenize_with_nltk(test_text)}\")\n",
                "print(f\"3. Simple: {tokenize_simple(test_text)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stemming (التجريد)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Stemming باستخدام ISRI:\n",
                        "\n",
                        "الكلمات الاصلية: ['الاكل', 'لذيذ', 'والجو', 'رائع', 'انصح', 'الجميع', 'بزياره', 'هذا', 'المطعم']\n",
                        "بعد التجريد: ['اكل', 'لذذ', 'لجو', 'رئع', 'نصح', 'جمع', 'زير', 'هذا', 'طعم']\n"
                    ]
                }
            ],
            "source": [
                "isri_stemmer = ISRIStemmer()\n",
                "\n",
                "def stem_with_isri(tokens):\n",
                "    \"\"\"Stemming باستخدام ISRI\"\"\"\n",
                "    return [isri_stemmer.stem(token) for token in tokens]\n",
                "\n",
                "tokens = tokenize_simple(cleaned_texts[2])\n",
                "\n",
                "print(\"Stemming باستخدام ISRI:\")\n",
                "print(f\"\\nالكلمات الاصلية: {tokens}\")\n",
                "print(f\"بعد التجريد: {stem_with_isri(tokens)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "جدول التجريد:\n",
                        "----------------------------------------\n",
                        "يذهبون -> ذهب\n",
                        "ذهب -> ذهب\n",
                        "ذاهب -> ذهب\n",
                        "مذهب -> ذهب\n",
                        "يكتبون -> كتب\n",
                        "كتب -> كتب\n",
                        "كاتب -> كتب\n",
                        "مكتوب -> كتب\n",
                        "المسلمون -> سلم\n",
                        "مسلم -> سلم\n",
                        "اسلام -> سلم\n",
                        "اسلامي -> سلم\n"
                    ]
                }
            ],
            "source": [
                "test_words = [\n",
                "    \"يذهبون\", \"ذهب\", \"ذاهب\", \"مذهب\",\n",
                "    \"يكتبون\", \"كتب\", \"كاتب\", \"مكتوب\",\n",
                "    \"المسلمون\", \"مسلم\", \"اسلام\", \"اسلامي\"\n",
                "]\n",
                "\n",
                "print(\"جدول التجريد:\")\n",
                "print(\"-\" * 40)\n",
                "for word in test_words:\n",
                "    stem = isri_stemmer.stem(word)\n",
                "    print(f\"{word} -> {stem}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Lemmatization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "مقارنة Lemmatization:\n",
                        "------------------------------------------------------------\n",
                        "المسلمون -> Simple: مسلم, ISRI: سلم\n",
                        "الكتب -> Simple: كتب, ISRI: كتب\n",
                        "يذهبون -> Simple: يذهب, ISRI: ذهب\n",
                        "المدرسه -> Simple: مدرس, ISRI: درس\n",
                        "الطلاب -> Simple: طلاب, ISRI: طلب\n"
                    ]
                }
            ],
            "source": [
                "def lemmatize_simple(word):\n",
                "    \"\"\"Lemmatization بسيط\"\"\"\n",
                "    prefixes = ['ال', 'و', 'ف', 'ب', 'ك', 'ل']\n",
                "    suffixes = ['ون', 'ين', 'ات', 'ان', 'ه', 'ي']\n",
                "    \n",
                "    for prefix in prefixes:\n",
                "        if word.startswith(prefix) and len(word) > len(prefix) + 2:\n",
                "            word = word[len(prefix):]\n",
                "            break\n",
                "    \n",
                "    for suffix in suffixes:\n",
                "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
                "            word = word[:-len(suffix)]\n",
                "            break\n",
                "    \n",
                "    return word\n",
                "\n",
                "test_words_lemma = [\"المسلمون\", \"الكتب\", \"يذهبون\", \"المدرسه\", \"الطلاب\"]\n",
                "\n",
                "print(\"مقارنة Lemmatization:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for word in test_words_lemma:\n",
                "    simple = lemmatize_simple(word)\n",
                "    stem = isri_stemmer.stem(word)\n",
                "    print(f\"{word} -> Simple: {simple}, ISRI: {stem}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## مقارنة المكتبات"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "نتائج المقارنة:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>text</th>\n",
                            "      <th>spacy_tokens</th>\n",
                            "      <th>nltk_tokens</th>\n",
                            "      <th>isri_stems</th>\n",
                            "      <th>simple_lemmas</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>مرحبا بكم في اليمن السعيد الخدمه روعه جدا</td>\n",
                            "      <td>[مرحبا, بكم, في, اليمن, السعيد, الخدمه, روعه, ...</td>\n",
                            "      <td>[مرحبا, بكم, في, اليمن, السعيد, الخدمه, روعه, ...</td>\n",
                            "      <td>[رحب, بكم, في, يمن, سعد, خدم, روع, جدا]</td>\n",
                            "      <td>[مرحبا, بكم, في, يمن, سعيد, خدم, روع, جدا]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>السعر ريال غالي جدا</td>\n",
                            "      <td>[السعر, ريال, غالي, جدا]</td>\n",
                            "      <td>[السعر, ريال, غالي, جدا]</td>\n",
                            "      <td>[سعر, ريل, غلي, جدا]</td>\n",
                            "      <td>[سعر, ريال, غال, جدا]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>الاكل لذيذ والجو رائع انصح الجميع بزياره هذا ا...</td>\n",
                            "      <td>[الاكل, لذيذ, والجو, رائع, انصح, الجميع, بزيار...</td>\n",
                            "      <td>[الاكل, لذيذ, والجو, رائع, انصح, الجميع, بزيار...</td>\n",
                            "      <td>[اكل, لذذ, لجو, رئع, نصح, جمع, زير, هذا, طعم]</td>\n",
                            "      <td>[اكل, ذيذ, الجو, رائع, انصح, جميع, زيار, هذا, ...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                text  \\\n",
                            "0          مرحبا بكم في اليمن السعيد الخدمه روعه جدا   \n",
                            "1                                السعر ريال غالي جدا   \n",
                            "2  الاكل لذيذ والجو رائع انصح الجميع بزياره هذا ا...   \n",
                            "\n",
                            "                                        spacy_tokens  \\\n",
                            "0  [مرحبا, بكم, في, اليمن, السعيد, الخدمه, روعه, ...   \n",
                            "1                           [السعر, ريال, غالي, جدا]   \n",
                            "2  [الاكل, لذيذ, والجو, رائع, انصح, الجميع, بزيار...   \n",
                            "\n",
                            "                                         nltk_tokens  \\\n",
                            "0  [مرحبا, بكم, في, اليمن, السعيد, الخدمه, روعه, ...   \n",
                            "1                           [السعر, ريال, غالي, جدا]   \n",
                            "2  [الاكل, لذيذ, والجو, رائع, انصح, الجميع, بزيار...   \n",
                            "\n",
                            "                                      isri_stems  \\\n",
                            "0        [رحب, بكم, في, يمن, سعد, خدم, روع, جدا]   \n",
                            "1                           [سعر, ريل, غلي, جدا]   \n",
                            "2  [اكل, لذذ, لجو, رئع, نصح, جمع, زير, هذا, طعم]   \n",
                            "\n",
                            "                                       simple_lemmas  \n",
                            "0         [مرحبا, بكم, في, يمن, سعيد, خدم, روع, جدا]  \n",
                            "1                              [سعر, ريال, غال, جدا]  \n",
                            "2  [اكل, ذيذ, الجو, رائع, انصح, جميع, زيار, هذا, ...  "
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def compare_libraries(texts):\n",
                "    \"\"\"مقارنة المكتبات المختلفة\"\"\"\n",
                "    results = {\n",
                "        'text': [],\n",
                "        'spacy_tokens': [],\n",
                "        'nltk_tokens': [],\n",
                "        'isri_stems': [],\n",
                "        'simple_lemmas': []\n",
                "    }\n",
                "    \n",
                "    for text in texts:\n",
                "        spacy_tok = tokenize_with_spacy(text)\n",
                "        nltk_tok = tokenize_with_nltk(text)\n",
                "        isri_stems = stem_with_isri(nltk_tok)\n",
                "        simple_lemmas = [lemmatize_simple(t) for t in nltk_tok]\n",
                "        \n",
                "        results['text'].append(text)\n",
                "        results['spacy_tokens'].append(spacy_tok)\n",
                "        results['nltk_tokens'].append(nltk_tok)\n",
                "        results['isri_stems'].append(isri_stems)\n",
                "        results['simple_lemmas'].append(simple_lemmas)\n",
                "    \n",
                "    return pd.DataFrame(results)\n",
                "\n",
                "comparison_df = compare_libraries(cleaned_texts[:3])\n",
                "print(\"نتائج المقارنة:\")\n",
                "comparison_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "قياس سرعة المكتبات:\n",
                        "----------------------------------------\n",
                        "Tokenization:\n",
                        "  Spacy: 5.34 ms\n",
                        "  NLTK: 0.10 ms\n",
                        "  Simple: 0.00 ms\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "def measure_speed(func, data, iterations=100):\n",
                "    \"\"\"قياس سرعة تنفيذ الدالة\"\"\"\n",
                "    start = time.time()\n",
                "    for _ in range(iterations):\n",
                "        for item in data:\n",
                "            func(item)\n",
                "    end = time.time()\n",
                "    return (end - start) / iterations\n",
                "\n",
                "print(\"قياس سرعة المكتبات:\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "spacy_time = measure_speed(tokenize_with_spacy, cleaned_texts)\n",
                "nltk_time = measure_speed(tokenize_with_nltk, cleaned_texts)\n",
                "simple_time = measure_speed(tokenize_simple, cleaned_texts)\n",
                "\n",
                "print(f\"Tokenization:\")\n",
                "print(f\"  Spacy: {spacy_time*1000:.2f} ms\")\n",
                "print(f\"  NLTK: {nltk_time*1000:.2f} ms\")\n",
                "print(f\"  Simple: {simple_time*1000:.2f} ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## تقرير المقارنة"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "# تقرير مقارنة ادوات معالجة النصوص العربية\n",
                        "\n",
                        "## 1. Tokenization\n",
                        "- Spacy: متوسطة السرعة، دقة عالية\n",
                        "- NLTK: سريعة، دقة جيدة\n",
                        "- Simple: سريعة جدا، دقة منخفضة\n",
                        "\n",
                        "الافضل: Spacy للمهام المتقدمة\n",
                        "\n",
                        "## 2. Stemming\n",
                        "- ISRI: سريعة، دقة جيدة للعربية\n",
                        "\n",
                        "الافضل: ISRI Stemmer\n",
                        "\n",
                        "## 3. Lemmatization\n",
                        "- Qalsadi: بطيئة، دقة عالية جدا\n",
                        "- Simple Rules: سريعة، دقة منخفضة\n",
                        "\n",
                        "الافضل: Qalsadi للدقة العالية\n",
                        "\n",
                        "## التوصيات\n",
                        "1. Tokenization: Spacy\n",
                        "2. Stemming: ISRI\n",
                        "3. Lemmatization: Qalsadi\n",
                        "4. التنظيف: pyarabic + regex\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "comparison_report = \"\"\"\n",
                "# تقرير مقارنة ادوات معالجة النصوص العربية\n",
                "\n",
                "## 1. Tokenization\n",
                "- Spacy: متوسطة السرعة، دقة عالية\n",
                "- NLTK: سريعة، دقة جيدة\n",
                "- Simple: سريعة جدا، دقة منخفضة\n",
                "\n",
                "الافضل: Spacy للمهام المتقدمة\n",
                "\n",
                "## 2. Stemming\n",
                "- ISRI: سريعة، دقة جيدة للعربية\n",
                "\n",
                "الافضل: ISRI Stemmer\n",
                "\n",
                "## 3. Lemmatization\n",
                "- Qalsadi: بطيئة، دقة عالية جدا\n",
                "- Simple Rules: سريعة، دقة منخفضة\n",
                "\n",
                "الافضل: Qalsadi للدقة العالية\n",
                "\n",
                "## التوصيات\n",
                "1. Tokenization: Spacy\n",
                "2. Stemming: ISRI\n",
                "3. Lemmatization: Qalsadi\n",
                "4. التنظيف: pyarabic + regex\n",
                "\"\"\"\n",
                "\n",
                "print(comparison_report)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## حفظ البيانات"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "البيانات النهائية:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>original_text</th>\n",
                            "      <th>cleaned_text</th>\n",
                            "      <th>tokens</th>\n",
                            "      <th>stems</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>مرحباااا بكم في اليمن السعيد الخدمة روووعة جدا</td>\n",
                            "      <td>مرحبا بكم في اليمن السعيد الخدمه روعه جدا</td>\n",
                            "      <td>[مرحبا, بكم, في, اليمن, السعيد, الخدمه, روعه, ...</td>\n",
                            "      <td>[رحب, بكم, في, يمن, سعد, خدم, روع, جدا]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>السعر 1500 ريال غالي جدا</td>\n",
                            "      <td>السعر ريال غالي جدا</td>\n",
                            "      <td>[السعر, ريال, غالي, جدا]</td>\n",
                            "      <td>[سعر, ريل, غلي, جدا]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>الاكل لذيذ والجو رائع انصح الجميع بزيارة هذا ا...</td>\n",
                            "      <td>الاكل لذيذ والجو رائع انصح الجميع بزياره هذا ا...</td>\n",
                            "      <td>[الاكل, لذيذ, والجو, رائع, انصح, الجميع, بزيار...</td>\n",
                            "      <td>[اكل, لذذ, لجو, رئع, نصح, جمع, زير, هذا, طعم]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>خدمةةةة سيئةةة جدااااا لن اعود مرة اخرى</td>\n",
                            "      <td>خدمه سيئه جدا لن اعود مره اخري</td>\n",
                            "      <td>[خدمه, سيئه, جدا, لن, اعود, مره, اخري]</td>\n",
                            "      <td>[خدم, سيئ, جدا, لن, اعد, مره, اخر]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>تجربة ممتازة 100 شكرا للفريق</td>\n",
                            "      <td>تجربه ممتازه شكرا للفريق</td>\n",
                            "      <td>[تجربه, ممتازه, شكرا, للفريق]</td>\n",
                            "      <td>[جرب, متز, شكر, فرق]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>المنتج وصل متاخر ب 3 ايام والتغليف سيء</td>\n",
                            "      <td>المنتج وصل متاخر ب ايام والتغليف سيء</td>\n",
                            "      <td>[المنتج, وصل, متاخر, ب, ايام, والتغليف, سيء]</td>\n",
                            "      <td>[نتج, وصل, تخر, ب, ايم, غلف, سيء]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>احببت الخدمه كثييييرا ساعود قريبا ان شاء الله</td>\n",
                            "      <td>احببت الخدمه كثيرا ساعود قريبا ان شاء الله</td>\n",
                            "      <td>[احببت, الخدمه, كثيرا, ساعود, قريبا, ان, شاء, ...</td>\n",
                            "      <td>[حبب, خدم, كثر, سعد, قرب, ان, شاء, الل]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>لا انصح ابدا التعامل سيء والاسعار مرتفعة جدا</td>\n",
                            "      <td>لا انصح ابدا التعامل سيء والاسعار مرتفعه جدا</td>\n",
                            "      <td>[لا, انصح, ابدا, التعامل, سيء, والاسعار, مرتفع...</td>\n",
                            "      <td>[لا, نصح, ابد, عمل, سيء, سعر, رفع, جدا]</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                       original_text  \\\n",
                            "0     مرحباااا بكم في اليمن السعيد الخدمة روووعة جدا   \n",
                            "1                           السعر 1500 ريال غالي جدا   \n",
                            "2  الاكل لذيذ والجو رائع انصح الجميع بزيارة هذا ا...   \n",
                            "3            خدمةةةة سيئةةة جدااااا لن اعود مرة اخرى   \n",
                            "4                       تجربة ممتازة 100 شكرا للفريق   \n",
                            "5             المنتج وصل متاخر ب 3 ايام والتغليف سيء   \n",
                            "6      احببت الخدمه كثييييرا ساعود قريبا ان شاء الله   \n",
                            "7       لا انصح ابدا التعامل سيء والاسعار مرتفعة جدا   \n",
                            "\n",
                            "                                        cleaned_text  \\\n",
                            "0          مرحبا بكم في اليمن السعيد الخدمه روعه جدا   \n",
                            "1                                السعر ريال غالي جدا   \n",
                            "2  الاكل لذيذ والجو رائع انصح الجميع بزياره هذا ا...   \n",
                            "3                     خدمه سيئه جدا لن اعود مره اخري   \n",
                            "4                           تجربه ممتازه شكرا للفريق   \n",
                            "5               المنتج وصل متاخر ب ايام والتغليف سيء   \n",
                            "6         احببت الخدمه كثيرا ساعود قريبا ان شاء الله   \n",
                            "7       لا انصح ابدا التعامل سيء والاسعار مرتفعه جدا   \n",
                            "\n",
                            "                                              tokens  \\\n",
                            "0  [مرحبا, بكم, في, اليمن, السعيد, الخدمه, روعه, ...   \n",
                            "1                           [السعر, ريال, غالي, جدا]   \n",
                            "2  [الاكل, لذيذ, والجو, رائع, انصح, الجميع, بزيار...   \n",
                            "3             [خدمه, سيئه, جدا, لن, اعود, مره, اخري]   \n",
                            "4                      [تجربه, ممتازه, شكرا, للفريق]   \n",
                            "5       [المنتج, وصل, متاخر, ب, ايام, والتغليف, سيء]   \n",
                            "6  [احببت, الخدمه, كثيرا, ساعود, قريبا, ان, شاء, ...   \n",
                            "7  [لا, انصح, ابدا, التعامل, سيء, والاسعار, مرتفع...   \n",
                            "\n",
                            "                                           stems  \n",
                            "0        [رحب, بكم, في, يمن, سعد, خدم, روع, جدا]  \n",
                            "1                           [سعر, ريل, غلي, جدا]  \n",
                            "2  [اكل, لذذ, لجو, رئع, نصح, جمع, زير, هذا, طعم]  \n",
                            "3             [خدم, سيئ, جدا, لن, اعد, مره, اخر]  \n",
                            "4                           [جرب, متز, شكر, فرق]  \n",
                            "5              [نتج, وصل, تخر, ب, ايم, غلف, سيء]  \n",
                            "6        [حبب, خدم, كثر, سعد, قرب, ان, شاء, الل]  \n",
                            "7        [لا, نصح, ابد, عمل, سيء, سعر, رفع, جدا]  "
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "final_df = pd.DataFrame({\n",
                "    'original_text': sample_texts,\n",
                "    'cleaned_text': cleaned_texts,\n",
                "    'tokens': [tokenize_simple(t) for t in cleaned_texts],\n",
                "    'stems': [stem_with_isri(tokenize_simple(t)) for t in cleaned_texts]\n",
                "})\n",
                "\n",
                "print(\"البيانات النهائية:\")\n",
                "final_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "تم حفظ البيانات في: data/cleaned_texts.csv\n",
                        "تم حفظ تقرير المقارنة في: comparison_report.md\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "os.makedirs('data', exist_ok=True)\n",
                "\n",
                "final_df.to_csv('data/cleaned_texts.csv', index=False, encoding='utf-8-sig')\n",
                "print(\"تم حفظ البيانات في: data/cleaned_texts.csv\")\n",
                "\n",
                "with open('comparison_report.md', 'w', encoding='utf-8') as f:\n",
                "    f.write(comparison_report)\n",
                "print(\"تم حفظ تقرير المقارنة في: comparison_report.md\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ملخص التكليف الثاني\n",
                "\n",
                "### ما تم انجازه:\n",
                "\n",
                "1. تنظيف النصوص:\n",
                "   - ازالة الايموجي\n",
                "   - ازالة علامات الترقيم\n",
                "   - ازالة المد والتكرار\n",
                "   - ازالة الارقام\n",
                "   - تطبيع النص العربي\n",
                "\n",
                "2. العمليات اللغوية:\n",
                "   - Tokenization (Spacy, NLTK, Simple)\n",
                "   - Stemming (ISRI Stemmer)\n",
                "   - Lemmatization\n",
                "\n",
                "3. مقارنة المكتبات\n",
                "\n",
                "4. المخرجات:\n",
                "   - data/cleaned_texts.csv\n",
                "   - comparison_report.md\n",
                "\n",
                "انتهى التكليف الثاني"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

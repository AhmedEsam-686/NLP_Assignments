{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ› ï¸ Ø§Ù„ØªÙƒÙ„ÙŠÙ Ø§Ù„Ø®Ø§Ù…Ø³: Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙˆØ¯ (Code Debugging)\n",
                "\n",
                "## ğŸ¯ Ø§Ù„Ù‡Ø¯Ù\n",
                "ÙØ­Øµ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø®Ø·Ø£ØŒ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³Ø¨Ø¨ØŒ ÙˆÙƒØªØ§Ø¨Ø© Ø§Ù„Ø­Ù„.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“‹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ù…Ø´Ø§Ø±ÙŠØ¹ NLP\n",
                "\n",
                "Ø³Ù†Ø³ØªØ¹Ø±Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ¸Ù‡Ø± ÙÙŠ Ù…Ø´Ø§Ø±ÙŠØ¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙƒÙŠÙÙŠØ© Ø¥ØµÙ„Ø§Ø­Ù‡Ø§."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1ï¸âƒ£ Ø®Ø·Ø£: Ù†Ù…ÙˆØ°Ø¬ Spacy ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âŒ Ø®Ø·Ø£: [E050] Can't find model 'ar_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n"
                    ]
                }
            ],
            "source": [
                "# âŒ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Ø·Ø¦\n",
                "import spacy\n",
                "\n",
                "# Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯\n",
                "try:\n",
                "    nlp = spacy.load(\"ar_core_news_sm\")  # Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹\n",
                "    print(\"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\")\n",
                "except OSError as e:\n",
                "    print(f\"âŒ Ø®Ø·Ø£: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: xx_ent_wiki_sm\n"
                    ]
                }
            ],
            "source": [
                "# âœ… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ­Ø­\n",
                "import spacy\n",
                "\n",
                "def load_spacy_model(model_name=\"xx_ent_wiki_sm\"):\n",
                "    \"\"\"\n",
                "    ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Spacy Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n",
                "    \"\"\"\n",
                "    try:\n",
                "        nlp = spacy.load(model_name)\n",
                "        print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}\")\n",
                "        return nlp\n",
                "    except OSError:\n",
                "        print(f\"âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„...\")\n",
                "        import subprocess\n",
                "        subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", model_name])\n",
                "        nlp = spacy.load(model_name)\n",
                "        return nlp\n",
                "\n",
                "# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø©\n",
                "nlp = load_spacy_model(\"xx_ent_wiki_sm\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2ï¸âƒ£ Ø®Ø·Ø£: Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„ØªØ±Ù…ÙŠØ² (Encoding)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âŒ Ø®Ø·Ø£: FileNotFoundError: [Errno 2] No such file or directory: 'arabic_file.csv'\n"
                    ]
                }
            ],
            "source": [
                "# âŒ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Ø·Ø¦\n",
                "import pandas as pd\n",
                "\n",
                "try:\n",
                "    # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø¹Ø±Ø¨ÙŠ Ø¨Ø¯ÙˆÙ† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ±Ù…ÙŠØ²\n",
                "    df = pd.read_csv('arabic_file.csv')  # Ù‚Ø¯ ÙŠØ³Ø¨Ø¨ Ù…Ø´Ø§ÙƒÙ„\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Ø®Ø·Ø£: {type(e).__name__}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# âœ… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ­Ø­\n",
                "import pandas as pd\n",
                "\n",
                "def read_arabic_csv(filepath):\n",
                "    \"\"\"\n",
                "    Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù CSV Ø¹Ø±Ø¨ÙŠ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ±Ù…ÙŠØ²\n",
                "    \"\"\"\n",
                "    encodings = ['utf-8', 'utf-8-sig', 'cp1256', 'iso-8859-6']\n",
                "    \n",
                "    for encoding in encodings:\n",
                "        try:\n",
                "            df = pd.read_csv(filepath, encoding=encoding)\n",
                "            print(f\"âœ… ØªÙ… Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨ØªØ±Ù…ÙŠØ²: {encoding}\")\n",
                "            return df\n",
                "        except (UnicodeDecodeError, UnicodeError):\n",
                "            continue\n",
                "    \n",
                "    raise ValueError(f\"ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ±Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©\")\n",
                "\n",
                "# Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:\n",
                "# df = read_arabic_csv('arabic_file.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3ï¸âƒ£ Ø®Ø·Ø£: ValueError ÙÙŠ TfidfVectorizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âŒ Ø®Ø·Ø£: AttributeError: 'NoneType' object has no attribute 'lower'\n"
                    ]
                }
            ],
            "source": [
                "# âŒ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Ø·Ø¦\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "texts = [\"Ù…Ø±Ø­Ø¨Ø§\", \"\", \"Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹\", None]  # ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙØ§Ø±Øº Ùˆ None\n",
                "\n",
                "try:\n",
                "    vectorizer = TfidfVectorizer()\n",
                "    X = vectorizer.fit_transform(texts)\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Ø®Ø·Ø£: {type(e).__name__}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­: (4, 4)\n"
                    ]
                }
            ],
            "source": [
                "# âœ… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ­Ø­\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "import pandas as pd\n",
                "\n",
                "def clean_texts_for_vectorization(texts):\n",
                "    \"\"\"\n",
                "    ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡Ø§Øª\n",
                "    \"\"\"\n",
                "    cleaned = []\n",
                "    for text in texts:\n",
                "        if text is None:\n",
                "            cleaned.append('')\n",
                "        elif isinstance(text, str):\n",
                "            cleaned.append(text.strip())\n",
                "        else:\n",
                "            cleaned.append(str(text))\n",
                "    return cleaned\n",
                "\n",
                "# Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…\n",
                "texts = [\"Ù…Ø±Ø­Ø¨Ø§\", \"\", \"Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹\", None]\n",
                "clean_texts = clean_texts_for_vectorization(texts)\n",
                "\n",
                "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙØ§Ø±ØºØ© Ø£Ùˆ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§\n",
                "clean_texts = [t if t else 'ÙØ§Ø±Øº' for t in clean_texts]\n",
                "\n",
                "vectorizer = TfidfVectorizer()\n",
                "X = vectorizer.fit_transform(clean_texts)\n",
                "print(f\"âœ… ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­: {X.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4ï¸âƒ£ Ø®Ø·Ø£: IndexError ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âŒ Ø®Ø·Ø£: ValueError: Expected 2D array, got scalar array instead:\n",
                        "array=Ù†Øµ Ø¬Ø¯ÙŠØ¯ Ù„Ù„ØªÙ†Ø¨Ø¤.\n",
                        "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n"
                    ]
                }
            ],
            "source": [
                "# âŒ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Ø·Ø¦\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "import numpy as np\n",
                "\n",
                "# Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ©\n",
                "X_train = np.random.rand(100, 10)\n",
                "y_train = np.random.randint(0, 2, 100)\n",
                "\n",
                "model = LogisticRegression()\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Ø®Ø·Ø£: ØªÙ…Ø±ÙŠØ± Ù†Øµ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…ØªØ¬Ù‡\n",
                "try:\n",
                "    new_text = \"Ù†Øµ Ø¬Ø¯ÙŠØ¯ Ù„Ù„ØªÙ†Ø¨Ø¤\"\n",
                "    prediction = model.predict(new_text)  # Ø®Ø·Ø£!\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Ø®Ø·Ø£: {type(e).__name__}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# âœ… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ­Ø­\n",
                "def predict_text(text, model, vectorizer):\n",
                "    \"\"\"\n",
                "    ØªÙ†Ø¨Ø¤ ØµØ­ÙŠØ­ Ù„Ù„Ù†Øµ Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
                "    \"\"\"\n",
                "    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Øµ ÙÙŠ Ù‚Ø§Ø¦Ù…Ø©\n",
                "    if isinstance(text, str):\n",
                "        text = [text]\n",
                "    \n",
                "    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡\n",
                "    X_new = vectorizer.transform(text)\n",
                "    \n",
                "    # Ø§Ù„ØªÙ†Ø¨Ø¤\n",
                "    prediction = model.predict(X_new)\n",
                "    \n",
                "    return prediction\n",
                "\n",
                "# Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµØ­ÙŠØ­:\n",
                "# result = predict_text(\"Ù†Øµ Ø¬Ø¯ÙŠØ¯\", model, vectorizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5ï¸âƒ£ Ø®Ø·Ø£: MemoryError Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø©"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# âŒ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Ø·Ø¦ (Ù‚Ø¯ ÙŠØ³Ø¨Ø¨ MemoryError)\n",
                "# ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©\n",
                "\n",
                "# vectorizer = TfidfVectorizer(max_features=100000)  # Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹\n",
                "# X = vectorizer.fit_transform(huge_dataset)  # Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ø®Ù…Ø©"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø©\n"
                    ]
                }
            ],
            "source": [
                "# âœ… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ­Ø­\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "def create_efficient_vectorizer(max_features=5000, min_df=5):\n",
                "    \"\"\"\n",
                "    Ø¥Ù†Ø´Ø§Ø¡ vectorizer ÙØ¹Ø§Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©\n",
                "    \"\"\"\n",
                "    vectorizer = TfidfVectorizer(\n",
                "        max_features=max_features,  # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰\n",
                "        min_df=min_df,              # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø©\n",
                "        max_df=0.95,                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø¬Ø¯Ø§Ù‹\n",
                "        dtype='float32'             # Ø§Ø³ØªØ®Ø¯Ø§Ù… float32 Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† float64\n",
                "    )\n",
                "    return vectorizer\n",
                "\n",
                "# Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… batch processing\n",
                "def batch_transform(texts, vectorizer, batch_size=1000):\n",
                "    \"\"\"\n",
                "    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª\n",
                "    \"\"\"\n",
                "    from scipy.sparse import vstack\n",
                "    \n",
                "    results = []\n",
                "    for i in range(0, len(texts), batch_size):\n",
                "        batch = texts[i:i+batch_size]\n",
                "        batch_vec = vectorizer.transform(batch)\n",
                "        results.append(batch_vec)\n",
                "    \n",
                "    return vstack(results)\n",
                "\n",
                "print(\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø©\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6ï¸âƒ£ Ø®Ø·Ø£: Shape Mismatch ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âŒ Ø®Ø·Ø£: ValueError: Found input variables with inconsistent numbers of samples: [100, 50]\n"
                    ]
                }
            ],
            "source": [
                "# âŒ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Ø·Ø¦\n",
                "import numpy as np\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "X = np.random.rand(100, 10)  # 100 Ø¹ÙŠÙ†Ø©\n",
                "y = np.random.randint(0, 2, 50)  # 50 ØªØµÙ†ÙŠÙ ÙÙ‚Ø·!\n",
                "\n",
                "try:\n",
                "    model = LogisticRegression()\n",
                "    model.fit(X, y)  # Ø®Ø·Ø£: Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Ø®Ø·Ø£: {type(e).__name__}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØµØ­ÙŠØ­Ø©: 100 Ø¹ÙŠÙ†Ø©\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# âœ… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ­Ø­\n",
                "def validate_data_shapes(X, y):\n",
                "    \"\"\"\n",
                "    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
                "    \"\"\"\n",
                "    if hasattr(X, 'shape'):\n",
                "        n_samples_X = X.shape[0]\n",
                "    else:\n",
                "        n_samples_X = len(X)\n",
                "    \n",
                "    n_samples_y = len(y)\n",
                "    \n",
                "    if n_samples_X != n_samples_y:\n",
                "        raise ValueError(\n",
                "            f\"Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚: X ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {n_samples_X} Ø¹ÙŠÙ†Ø©ØŒ \"\n",
                "            f\"Ù„ÙƒÙ† y ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {n_samples_y} Ø¹ÙŠÙ†Ø©\"\n",
                "        )\n",
                "    \n",
                "    print(f\"âœ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØµØ­ÙŠØ­Ø©: {n_samples_X} Ø¹ÙŠÙ†Ø©\")\n",
                "    return True\n",
                "\n",
                "# Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…\n",
                "X = np.random.rand(100, 10)\n",
                "y = np.random.randint(0, 2, 100)\n",
                "validate_data_shapes(X, y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“‹ Ù‚Ø§Ù„Ø¨ ØªÙ‚Ø±ÙŠØ± Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙˆØ¯"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ù„Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: fix_report_template.md\n"
                    ]
                }
            ],
            "source": [
                "fix_report_template = \"\"\"\n",
                "# ğŸ“‹ ØªÙ‚Ø±ÙŠØ± Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙˆØ¯\n",
                "\n",
                "## Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø·Ø£\n",
                "\n",
                "| Ø§Ù„Ø­Ù‚Ù„ | Ø§Ù„Ù‚ÙŠÙ…Ø© |\n",
                "|-------|--------|\n",
                "| **Ù†ÙˆØ¹ Ø§Ù„Ø®Ø·Ø£** | [Ø§Ø³Ù… Ø§Ù„Ø®Ø·Ø£] |\n",
                "| **Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£** | [Ù†Øµ Ø§Ù„Ø±Ø³Ø§Ù„Ø©] |\n",
                "| **Ø§Ù„Ø³Ø·Ø±** | [Ø±Ù‚Ù… Ø§Ù„Ø³Ø·Ø±] |\n",
                "| **Ø§Ù„Ù…Ù„Ù** | [Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù] |\n",
                "\n",
                "---\n",
                "\n",
                "## Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø§Ù„Ø®Ø§Ø·Ø¦)\n",
                "\n",
                "```python\n",
                "# Ø§Ù„ÙƒÙˆØ¯ Ù‡Ù†Ø§\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## Ø³Ø¨Ø¨ Ø§Ù„Ø®Ø·Ø£\n",
                "\n",
                "[Ø´Ø±Ø­ Ø³Ø¨Ø¨ Ø­Ø¯ÙˆØ« Ø§Ù„Ø®Ø·Ø£]\n",
                "\n",
                "---\n",
                "\n",
                "## Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ­Ø­\n",
                "\n",
                "```python\n",
                "# Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ­Ø­ Ù‡Ù†Ø§\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥ØµÙ„Ø§Ø­\n",
                "\n",
                "- [ ] ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ù†Ø¬Ø§Ø­\n",
                "- [ ] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡\n",
                "- [ ] Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØµØ­ÙŠØ­Ø©\n",
                "\"\"\"\n",
                "\n",
                "# Ø­ÙØ¸ Ø§Ù„Ù‚Ø§Ù„Ø¨\n",
                "with open('fix_report_template.md', 'w', encoding='utf-8') as f:\n",
                "    f.write(fix_report_template)\n",
                "\n",
                "print(\"âœ… ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ù„Ø¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: fix_report_template.md\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“Œ Ù…Ù„Ø®Øµ Ø§Ù„ØªÙƒÙ„ÙŠÙ Ø§Ù„Ø®Ø§Ù…Ø³\n",
                "\n",
                "### Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªÙŠ ØªÙ… ØªØºØ·ÙŠØªÙ‡Ø§:\n",
                "\n",
                "| # | Ø§Ù„Ø®Ø·Ø£ | Ø§Ù„Ø³Ø¨Ø¨ | Ø§Ù„Ø­Ù„ |\n",
                "|---|-------|-------|------|\n",
                "| 1 | OSError: Model not found | Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø«Ø¨Øª | ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ |\n",
                "| 2 | UnicodeDecodeError | ØªØ±Ù…ÙŠØ² Ø®Ø§Ø·Ø¦ | ØªØ¬Ø±Ø¨Ø© ØªØ±Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© |\n",
                "| 3 | ValueError: empty vocabulary | Ù†ØµÙˆØµ ÙØ§Ø±ØºØ© | ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª |\n",
                "| 4 | IndexError | Ù†ÙˆØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§Ø·Ø¦ | ØªØ­ÙˆÙŠÙ„ ØµØ­ÙŠØ­ Ù„Ù„Ù…ØªØ¬Ù‡Ø§Øª |\n",
                "| 5 | MemoryError | Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ø®Ù…Ø© | batch processing |\n",
                "| 6 | Shape mismatch | Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ø£Ø¨Ø¹Ø§Ø¯ | Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª |\n",
                "\n",
                "### Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:\n",
                "- `fix_report_template.md` - Ù‚Ø§Ù„Ø¨ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¥ØµÙ„Ø§Ø­\n",
                "\n",
                "---\n",
                "\n",
                "**âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªÙƒÙ„ÙŠÙ Ø§Ù„Ø®Ø§Ù…Ø³**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
